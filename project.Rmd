---
title: "Projet STA203"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
authors: COULIBALY Mouhamed, TAJJA Ayoub
---
## Introduction 
L'objectif de ce projet est d'arriver à s'affranchir des analyses chimiques côuteuses pour pouvoir déterminer la teneur en en sucre. Pour cela, nous construirons un modèle d'apprentissage statistique en nous basant sur un dataset contenant des mesures de spectrométrie infrarouge contenu dans le jeu de données \textit{cookies}.

## Etude théorique 
bdzvy

## Analyse Exploratoire 
Pour nous familiariser avec le jeu de donnée on va faire une analyse exploratoire du dataset. 
```{r echo=FALSE}
rm(list = ls())
graphics.off()
require(latex2exp)
```

```{r echo=FALSE}
load('cookie.app.RData')
load('cookie.val.RData')

#Lecture des données

xtrain = cookie.app[-1]
dim(xtrain)
```
On a donc un dataset avec 700 variables qui représentent les fréquences auxquelles on mesure l'absorbance et 40 individus. 

```{r echo=FALSE}
head(xtrain)
```


```{r echo=FALSE}
ytrain = cookie.app[1]
head(ytrain)
```

```{r echo=FALSE}
xtest = cookie.val[-1]
head(xtest)
```
```{r echo=FALSE}
ytest = cookie.val[1]
head(ytrain)
```

On va tracer les boxplots des variables explicatives pour étudier les variabilités.
```{r echo=FALSE}
par(mfrow=c(3,2))
boxplot(xtrain[1:50])
boxplot(xtrain[51:100])
boxplot(xtrain[101:150])
boxplot(xtrain[201:250])
boxplot(xtrain[251:300])
boxplot(xtrain[301:350])

```
On peut voir que les variables n'ont pas la même variabilité (les médianes sont très différentes). Pour faire une bonne études exploratoire il faudra centrer et réduire le dataset.
Maitenant affichons les spectres pour voir(Par soucis de clarté nous allons le faire que pour 2 individus mais le code a été testé sur les 40 individus et les conclusions sont les même).
```{r echo=FALSE}
par(mfrow=c(1,3))
sapply( 1:3, function(i){
                    matplot(1:700, t(xtrain[i,]), type = 'l',
                            main = paste("Spectre de l'individu: ", i," .")
                            , xlab="Fréquence dans le proche infra-rouge"
                            , ylab="Absorbance") })

```
L'allure des spectre est la même pour tous les individus. Les comportements sont assez similaires i.e on retrouve les minima et maxima locaux dans pour les même fréquences. 
Intéressons nous maintenant à corrélations entre les différentes fréquences. Pour cela faisons un corrplot. Face au nombre élevé de variables (700) la représentation graphique est impossible pour donc faire une interprétation graphique intéressante nous allons pas faire le corrplot sur des groupes de fréquences 'éloignées'. En effet, l'allure des spectre nous montre qu'on a une certaine continuité de l'absorbance par rapport à la fréquence. Ainsi pour des fréquences proches sont forcément trés corrélées par continuité. Il est donc plus intéressant de voir la corrélation entre des variables avec des gammes de fréquence 'éloignée'. Ainsi en divisant les fréquences en trois groupes et en choisissant 5 fréquences dans chaque groupe on observe:
```{r echo=FALSE}
library(corrplot)
M =cbind(xtrain[,1:5],xtrain[,300:305],xtrain[,680:685])
corrplot(cor(M))
#C <- cor(xtrain)

```
On remarque bien que notre hypothèse de continuité est vérifiée car la corrélation entre les fréquences proches est proche de 1 alors que pour des fréquences plus éloignées la corrélation est plus faible.
On effectue maintenant une ACP sur le dataset.
```{r echo=FALSE}
library(FactoMineR)
res.acp = PCA(xtrain, ncp = 700,graph=F)
barplot(res.acp$eig[,1] / sum(res.acp$eig[,1]) * 100, las = 2)
abline(h = 100 / 700, col = "red")
#which(res.pca$eig[,3] > 100 - 1/7)
```
La PCA se fait que sur 39 variable ceci est due au fait que la matrice $X'X$ n'est pas injective, on se trouve bien dans le cadre discuté en première partie.
La représentation des nuages de points sur les 6 axes principaux donne:
```{r echo=FALSE}
par(mfrow = c(1, 2))
plot(res.acp, axes = c(1, 2), choix = "ind",
     graph.type="classic")

plot(res.acp, axes = c(1, 2), choix = "var",
     graph.type="classic")

plot(res.acp, axes = c(2, 3), choix = "ind",
     graph.type="classic")

plot(res.acp, axes = c(2, 3), choix = "var",
     graph.type="classic")
```

```{r echo=FALSE}
par(mfrow = c(2, 2))

plot(res.acp, axes = c(3, 4), choix = "ind",
     graph.type="classic")

plot(res.acp, axes = c(3, 4), choix = "var",
     graph.type="classic")

plot(res.acp, axes = c(4, 5), choix = "ind",
     graph.type="classic")

plot(res.acp, axes = c(4, 5), choix = "var",
     graph.type="classic")
```

```{r echo=FALSE}
par(mfrow = c(1, 2))
plot(res.acp, axes = c(5, 6), choix = "ind",
     graph.type="classic")

plot(res.acp, axes = c(5, 6), choix = "var",
     graph.type="classic")
```
On retrouve bien le fait que les variables peuvent être représentées avec les deux premières dimensions uniquement.
## Reconstruction du nuage de points 
Nous allons 
```{r echo=FALSE}
reconstruct <- function(res, nr, Xm, Xsd){
  Fs <- as.matrix(res$ind$coord[,1:nr])
  us <- as.matrix(res$svd$V[,1:nr])
  rep <- Fs[,1]%*%t(us[,1])
  if(nr > 1){
    for( i in 2:nr){
      rep <- rep + Fs[,i]%*%t(us[,i])
    }
  }
  rep<- t(apply(rep, 1, function(x){x*Xsd+Xm}))
  return(rep)
}
```
On vérifie que la fonction avec xtrain:
```{r echo=FALSE}
Xm <- apply(xtrain, 2, mean)
Xsd <- apply(xtrain, 2, function(x){sqrt((var(x)*(length(x)-1))/length(x))})

reconst_tot <- reconstruct(res.acp, 39, Xm, Xsd)

RMSE <- sum((reconst_tot - xtrain)^2)/28000
MAE <- sum(abs(reconstruct(res.acp, 39, Xm, Xsd) - xtrain))/28000


plot(1:700,reconstruct(res.acp, 39, Xm, Xsd)[1,], type = 'l',
     main = paste("Reconstruction complète; RMSE =", RMSE, "MAE =", MAE,"."),
     xlab = "Fréquence (pas en HZ)",
     ylab = "Amplitude")
for(i in 2:40){
  lines(1:700, reconstruct(res.acp, 39, Xm, Xsd)[i,], type = 'l')
}
```
On a bien les même spectres ( les erreurs quadratiques moyennes et en valeur absolue sont raisonnables ) la reconstruction est donc bonne.
On fait la reconstruction pour un nombre d'axes donné:
```{r echo=FALSE}
par(mfrow = c(3, 2))
nr = c(1, 2, 3, 4, 5, 39)
for(i in nr){
  X <- reconstruct(res.acp, i, Xm, Xsd)
  RMSE <- round(sqrt(sum((X - xtrain)^2)/28000), 5)
  MAE <- round(sum(abs(X - xtrain))/28000, 5)
  plot(1:700,X[1,], type = 'l',
       main = paste("Reconstruction pour nr =", i, "; RMSE =", RMSE, "MAE =", MAE,"."),
       xlab = "Fréquence (pas en HZ)",
       ylab = "Amplitude")
  for(j in 2:nrow(X)){
    lines(1:700,X[j,], type = 'l')
  }
}
```
## Régression pénalisée 
### Question 3.1
Notre dataset est constitué de variables très corrélées ceci peut fausser la résolution numérique. C'est pourquoi on effectue une régression avec le modèle \textit{ridge} avec la fonction \textit{glmet}. Ceci consiste à optimiser le problème 
```{r echo=FALSE}
library(glmnet)
library(MASS)
grid= 10^seq(6,-10,length=100)
ridge.fit <-glmnet(x = xtrain, y = unlist(ytrain),alpha=0 ,lambda=grid)
plot(log10(grid), coef(ridge.fit)[1,], type='l', xlab='grid (échelle logarithmique)', ylab='intercept', main='Variation de l\'estimation de l\'intercept', xlim = c(-10, 6), ylim = c(-10, 40))
```
L'estimateur de l'intercept n'est pas pénalisé et dépend grandement de la valeur des autres paramètres estimés qui eux sont étroitement liés à la valeur du paramètre de pénalisation. Quand le  paramètre de pénalisation tend vers zéro on voit bien qu'on converge vers la limite en zéro du modèle. On trouve bien le comportement prédit par la théorie. 
```{r echo=FALSE}
xtrain = as.matrix(xtrain)
ytrain = as.matrix(ytrain)
X.mean = apply(xtrain, 2,mean)
mean(ytrain) - X.mean %*% coef(ridge.fit)[-1,1]
coef(ridge.fit)[1,1]
```
**Pas compris Ayoub si tu peux expliquer.........**
Intéressons nous maintenant aux cas où \textit{xtrain} et/ou \textit{ytrain} sont centrées:
Dans le cas où \textit{ytrain} est centrée
```{r echo=FALSE}
y.centered = ytrain - mean(ytrain)
ridge.fit =glmnet(x = xtrain, y = unlist(y.centered),alpha=0 ,lambda=grid)
plot(log10(grid), coef(ridge.fit)[1,], type='l', xlab='grid (échelle logarithmique)', ylab='intercept', main='Variation de l\'estimation de l\'intercept avec y variable centrée', xlim = c(-10, 6), ylim = c(-30, 30))

```
Lorsque l'on centre \textit{ytrain} on translate simplement la courbe obtenue avec le x choisi (\textit{xtrain} ou xtrain centré) vers le bas de la moyenne empirique de \textit{ytrain}.
```{r echo=FALSE}
x.centered = scale(xtrain) 
ridge.fit <-glmnet(x = x.centered, y = unlist(ytrain),alpha=0 ,lambda=grid)
plot(log10(grid), coef(ridge.fit)[1,], type='l', xlab='grid (échelle logarithmique)', ylab='intercept', main='Variation de l\'estimation de l\'intercept avec xtrain variable centrée', xlim = c(-10, 6), ylim = c(0, 30))
```
Dans le cas ou \textit{xtrain} est centrée. On a l'intercept qui est égal à la moyenne empirique de \texit{ytrain}  ce qui est normal.

Dans le cas où les deux sont centrées on a:
```{r echo=FALSE}
ridge.fit <-glmnet(x = x.centered, y = unlist(y.centered),alpha=0 ,lambda=grid)
plot(log10(grid), coef(ridge.fit)[1,], type='l', xlab='grid (échelle logarithmique)', ylab='intercept', main='Variation de l\'estimation de l\'intercept avec xtrain variable centrée', xlim = c(-10, 6), ylim = c(0, 10))
```
On retrouve bien un intercept nul comme attendu.
Dans le cas où les variables sont centrées réduites on a:
```{r echo=FALSE}
x.stdcentered = scale(xtrain,scale = TRUE)
y.stdcentered = scale(ytrain,scale = TRUE )
ridge.fit =glmnet(x = x.stdcentered, y = unlist(y.stdcentered),alpha=0 ,lambda=grid)
sigma = sqrt(eigen(t(x.stdcentered)%*%x.stdcentered)$values[1:39])
v = eigen(t(x.stdcentered)%*%x.stdcentered)$vectors[,1:39]
u = eigen(x.stdcentered%*%t(x.stdcentered))$vectors[,1:39]

theta <- 0
for(i in 1:39){
  theta = theta + (v[,i]%*%t(u[,i]))/sigma[i]
}

theta = theta %*%y.stdcentered
theta0 =mean(ytrain) - X.mean %*% theta
dim(theta)
theta0
```

### Question 3.2

```{r echo=FALSE}
out.ridge = lm.ridge( sucres ~ ., data=cookie.app, lambda=grid )
theta_lm_ridge = as.matrix(coef(out.ridge)[100,])

theta_glmnet = as.matrix(coef(ridge.fit)[,100])
theta_theo = as.matrix(c(theta0, theta))

paste('Les normes des theta estimés quand lambda tend vers 0:','Modèle de Ridge avec lm:', norm(theta_lm_ridge), ' , ','Modèle de Ridge avec glmnet:', norm(theta_glmnet), ' , ','Valeur théorique:', norm(theta_theo))
```
Nous avons pas les même valeurs pour les différents modèles. Ce qu'on ne parvient pas à expliquer 
### Question 3.3
On a vu grace au graphe de l'évolution de l'intercept en fonction de $\lambda$ que l'intercept se stabilisait un peu avant $10^{-5}$. De plus on peut se limiter à $10^{1}$ comme borne sup au vu du même graphe (on ne peut pas converger aprés). 

```{r echo=FALSE}
require(pls)
grid = 10^seq(1,-5,length=100)
set.seed(50)
X = as.matrix(xtrain)
Y = as.matrix(ytrain)
M = 4
folds = cvsegments(nrow(X), M, type="random")

errors = matrix(NA, M, length(grid))

for (b in 1:M)
{
  subsetb = unlist(folds[[b]])
  ridge.res = glmnet(X[-subsetb,], Y[-subsetb,], alpha=0, lambda=grid)
  # On cherhce le modèle avec la plus faible MSE
  for (j in 1:length(grid))
  {
    coef = coef(ridge.res)[,j]
    pred = cbind(1,X[subsetb,])%*%coef
    errors[b, j] = mean( (Y[subsetb,] - pred)^2 )
  }
}
#On cherche le lambda optimal
lambd.opt = grid[which.min(apply(errors, 2, mean))]

```
Affichons les résultats 
```{r echo=FALSE}
q=qnorm(0.85) #quantille pour le calcul de l'erreur 

plot(log(grid), apply(errors, 2, mean),type = 'l', col='red', lwd=3, xlab='Log(lambda)', ylab='Erreur moyenne ', ylim = c(2,12))
segments(x0=log(grid), y0=apply(errors, 2, function(err) mean(err) - q * sd(err) / sqrt(M)),
         x1=log(grid), y1=apply(errors, 2, function(err) mean(err) + q * sd(err) / sqrt(M)),
         col='blue')
```
Confrontons ce résultats avec ceux obtenus avec la fonction \textit{cv.glmnet}. On a:

```{r}
foldid <- rep(0, 40)
foldid[unlist(folds[[1]])]=1
foldid[unlist(folds[[2]])]=2
foldid[unlist(folds[[3]])]=3
foldid[unlist(folds[[4]])]=4

out.cv.ridge = cv.glmnet(X, Y, alpha=0, lambda=grid, type.measure='mse', nfolds=M, foldid = foldid)
plot(out.cv.ridge)
```
On obtient globalement le même comportement  (à un facteur prés) ce qui est rassurant. La différence sur les intervalles de confiance est due à l'estimateur choisi pour la variance  et aussi au choix du niveau des intervalles.
On choisi le $\lambda_{optimal}$ qui  minimise la MSE. Effectuons nos calculs avec ce $\lambda_{optimal}$ on obtient:
```{r}
lm.ridge = glmnet(X, Y, alpha=0, lambda=lambd.opt)
preds = predict(lm.ridge, newx=as.matrix(xtest))
err.gen <- mean( (preds - ytest)^2  )

paste('Erreur de généralisation MSE:', round(err.gen, 4))
paste('Erreur de généralisation relative (maximal):', round(err.gen/min(ytest), 4))
```
Nous avons un problème avec les erreurs on ne sait pas pourquoi...
# Regression logistique pénalisée
## Q2
```{r}
otrain = ytrain > 18
otest = ytest > 18
```
Regardons la distribution:
```{r}
table(otrain)
table(otest)
```
```{r}
grid = 10^seq(6,-10,length=100)
cv.out_ridge = cv.glmnet(X, otrain, alpha=0, family = binomial('logit'), nfolds=M, lambda=grid, foldid = foldid)
cv.out_lasso = cv.glmnet(X, otrain, alpha=1, family = binomial('logit'), nfolds=M, lambda=grid, foldid = foldid)

theta_r <- coef(cv.out_ridge, s = "lambda.min")
lambda_r <- cv.out_ridge$lambda.min

plot(cv.out_ridge, main='Ridge')
plot(cv.out_lasso, main='Lasso')
```
### Question 3
```{r }
ind_s <- function(x,s) ifelse(x>=s, 1,0)

ROC = function(x,theta,z){
  prediction = 1/ (1 + exp( - cbind(1, x) %*% as.matrix(theta)))
  new_l=NULL
  for(s in seq(0,1,0.001)){
    res = table((ind_s(prediction,s)),z)
    if (dim(res)[1]==1){
      l=(list(x=0, y=0))
    }
    else{
      alpha = res[2,2]/sum(res[,2])
      beta = res[1,1]/sum(res[,1])
      l=list(x=1-beta,y=alpha)
    }
    new_l$x=cbind(new_l$x,l$x)
    new_l$y=cbind(new_l$y,l$y)
  }
  return (new_l)
}
```

```{r}
theta.r = coef(cv.out_ridge, s = "lambda.min")
theta.l = coef(cv.out_lasso, s = "lambda.min")
#z = ytrain > 18
#ztest = ytest > 18
new_l = ROC(xtrain,theta.r,otrain)
plot(new_l, main='ROC Ridge train data', xlab='1-beta', ylab='alpha',type='b',col='black')
new_l = ROC(as.matrix(xtest),theta.r,otest)
plot(new_l, main='ROC Ridge test data', xlab='1-beta', ylab='alpha',type='b',col='black')

```

```{r}
new_l = ROC(xtrain,theta.l,z)
plot(new_l, main='ROC  train data', xlab='1-beta', ylab='alpha',type='b',col='red')
new_l = ROC(as.matrix(xtest),theta.l,otest)
plot(new_l, main='ROC  test data', xlab='1-beta', ylab='alpha',type='b',col='red')
```
Le modèle retenu en lasso est quant à lui très mauvais même en apprentissage. La courbe de ROC en test est très proche d'un modèle aléatoire.
